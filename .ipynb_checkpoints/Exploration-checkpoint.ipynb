{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c048640a-c701-497f-8234-f7200076791d",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89fb11cf-2eb1-4dd3-8ab6-5b79fb09d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "IMAGES_DIR = DATA_DIR / \"Images\"\n",
    "ANNOTATIONS_DIR = DATA_DIR / \"Annotation\"\n",
    "LISTS_DIR = DATA_DIR / \"Lists\"\n",
    "\n",
    "TARGET_IMG_DIR = Path(\"working_data/images\")\n",
    "TARGET_ANN_DIR = Path(\"working_data/annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b608c-855d-471c-985a-90b213cc287e",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28138b6a-e273-4e1c-8e5b-0bc2b5179745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02085620-Chihuahua: 152 images\n",
      "n02085782-Japanese_spaniel: 185 images\n",
      "n02085936-Maltese_dog: 252 images\n",
      "n02086079-Pekinese: 149 images\n",
      "n02086240-Shih-Tzu: 214 images\n",
      "n02086646-Blenheim_spaniel: 188 images\n",
      "n02086910-papillon: 196 images\n",
      "n02087046-toy_terrier: 172 images\n",
      "n02087394-Rhodesian_ridgeback: 172 images\n",
      "n02088094-Afghan_hound: 239 images\n",
      "n02088238-basset: 175 images\n",
      "n02088364-beagle: 195 images\n",
      "n02088466-bloodhound: 187 images\n",
      "n02088632-bluetick: 171 images\n",
      "n02089078-black-and-tan_coonhound: 159 images\n",
      "n02089867-Walker_hound: 153 images\n",
      "n02089973-English_foxhound: 157 images\n",
      "n02090379-redbone: 148 images\n",
      "n02090622-borzoi: 151 images\n",
      "n02090721-Irish_wolfhound: 218 images\n",
      "n02091032-Italian_greyhound: 182 images\n",
      "n02091134-whippet: 187 images\n",
      "n02091244-Ibizan_hound: 188 images\n",
      "n02091467-Norwegian_elkhound: 196 images\n",
      "n02091635-otterhound: 151 images\n",
      "n02091831-Saluki: 200 images\n",
      "n02092002-Scottish_deerhound: 232 images\n",
      "n02092339-Weimaraner: 160 images\n",
      "n02093256-Staffordshire_bullterrier: 155 images\n",
      "n02093428-American_Staffordshire_terrier: 164 images\n",
      "n02093647-Bedlington_terrier: 182 images\n",
      "n02093754-Border_terrier: 172 images\n",
      "n02093859-Kerry_blue_terrier: 179 images\n",
      "n02093991-Irish_terrier: 169 images\n",
      "n02094114-Norfolk_terrier: 172 images\n",
      "n02094258-Norwich_terrier: 185 images\n",
      "n02094433-Yorkshire_terrier: 164 images\n",
      "n02095314-wire-haired_fox_terrier: 157 images\n",
      "n02095570-Lakeland_terrier: 197 images\n",
      "n02095889-Sealyham_terrier: 202 images\n",
      "n02096051-Airedale: 202 images\n",
      "n02096177-cairn: 197 images\n",
      "n02096294-Australian_terrier: 196 images\n",
      "n02096437-Dandie_Dinmont: 180 images\n",
      "n02096585-Boston_bull: 182 images\n",
      "n02097047-miniature_schnauzer: 154 images\n",
      "n02097130-giant_schnauzer: 157 images\n",
      "n02097209-standard_schnauzer: 155 images\n",
      "n02097298-Scotch_terrier: 158 images\n",
      "n02097474-Tibetan_terrier: 206 images\n",
      "n02097658-silky_terrier: 183 images\n",
      "n02098105-soft-coated_wheaten_terrier: 156 images\n",
      "n02098286-West_Highland_white_terrier: 169 images\n",
      "n02098413-Lhasa: 186 images\n",
      "n02099267-flat-coated_retriever: 152 images\n",
      "n02099429-curly-coated_retriever: 151 images\n",
      "n02099601-golden_retriever: 150 images\n",
      "n02099712-Labrador_retriever: 171 images\n",
      "n02099849-Chesapeake_Bay_retriever: 167 images\n",
      "n02100236-German_short-haired_pointer: 152 images\n",
      "n02100583-vizsla: 154 images\n",
      "n02100735-English_setter: 161 images\n",
      "n02100877-Irish_setter: 155 images\n",
      "n02101006-Gordon_setter: 153 images\n",
      "n02101388-Brittany_spaniel: 152 images\n",
      "n02101556-clumber: 150 images\n",
      "n02102040-English_springer: 159 images\n",
      "n02102177-Welsh_springer_spaniel: 150 images\n",
      "n02102318-cocker_spaniel: 159 images\n",
      "n02102480-Sussex_spaniel: 151 images\n",
      "n02102973-Irish_water_spaniel: 150 images\n",
      "n02104029-kuvasz: 150 images\n",
      "n02104365-schipperke: 154 images\n",
      "n02105056-groenendael: 150 images\n",
      "n02105162-malinois: 150 images\n",
      "n02105251-briard: 152 images\n",
      "n02105412-kelpie: 153 images\n",
      "n02105505-komondor: 154 images\n",
      "n02105641-Old_English_sheepdog: 169 images\n",
      "n02105855-Shetland_sheepdog: 157 images\n",
      "n02106030-collie: 153 images\n",
      "n02106166-Border_collie: 150 images\n",
      "n02106382-Bouvier_des_Flandres: 150 images\n",
      "n02106550-Rottweiler: 152 images\n",
      "n02106662-German_shepherd: 152 images\n",
      "n02107142-Doberman: 150 images\n",
      "n02107312-miniature_pinscher: 184 images\n",
      "n02107574-Greater_Swiss_Mountain_dog: 168 images\n",
      "n02107683-Bernese_mountain_dog: 218 images\n",
      "n02107908-Appenzeller: 151 images\n",
      "n02108000-EntleBucher: 202 images\n",
      "n02108089-boxer: 151 images\n",
      "n02108422-bull_mastiff: 156 images\n",
      "n02108551-Tibetan_mastiff: 152 images\n",
      "n02108915-French_bulldog: 159 images\n",
      "n02109047-Great_Dane: 156 images\n",
      "n02109525-Saint_Bernard: 170 images\n",
      "n02109961-Eskimo_dog: 150 images\n",
      "n02110063-malamute: 178 images\n",
      "n02110185-Siberian_husky: 192 images\n",
      "n02110627-affenpinscher: 150 images\n",
      "n02110806-basenji: 209 images\n",
      "n02110958-pug: 200 images\n",
      "n02111129-Leonberg: 210 images\n",
      "n02111277-Newfoundland: 195 images\n",
      "n02111500-Great_Pyrenees: 213 images\n",
      "n02111889-Samoyed: 218 images\n",
      "n02112018-Pomeranian: 219 images\n",
      "n02112137-chow: 196 images\n",
      "n02112350-keeshond: 158 images\n",
      "n02112706-Brabancon_griffon: 153 images\n",
      "n02113023-Pembroke: 181 images\n",
      "n02113186-Cardigan: 155 images\n",
      "n02113624-toy_poodle: 151 images\n",
      "n02113712-miniature_poodle: 155 images\n",
      "n02113799-standard_poodle: 159 images\n",
      "n02113978-Mexican_hairless: 155 images\n",
      "n02115641-dingo: 156 images\n",
      "n02115913-dhole: 150 images\n",
      "n02116738-African_hunting_dog: 169 images\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for folder in IMAGES_DIR.iterdir():\n",
    "    if folder.is_dir():\n",
    "        image_count = len(list(folder.glob(\"*.*\"))) \n",
    "        print(f\"{folder.name}: {image_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "973c23c5-8ca7-4022-8702-dd70869679c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total breeds: 120\n",
      "Selected Breeds:\n",
      " ['n02091635-otterhound', 'n02109047-Great_Dane', 'n02107683-Bernese_mountain_dog', 'n02097047-miniature_schnauzer', 'n02100735-English_setter', 'n02108000-EntleBucher', 'n02091467-Norwegian_elkhound', 'n02107142-Doberman', 'n02090379-redbone', 'n02086240-Shih-Tzu', 'n02087394-Rhodesian_ridgeback', 'n02098105-soft-coated_wheaten_terrier', 'n02092339-Weimaraner', 'n02116738-African_hunting_dog', 'n02108422-bull_mastiff', 'n02098413-Lhasa', 'n02112018-Pomeranian', 'n02109525-Saint_Bernard', 'n02106166-Border_collie', 'n02102480-Sussex_spaniel']\n"
     ]
    }
   ],
   "source": [
    "all_breeds = sorted([d.name for d in IMAGES_DIR.iterdir() if d.is_dir()])\n",
    "print(f\"Total breeds: {len(all_breeds)}\")\n",
    "\n",
    "random.seed(739)\n",
    "\n",
    "# Reduced to 20 for Trial 3 from 30\n",
    "selected_breeds = random.sample(all_breeds, 20)\n",
    "print(\"Selected Breeds:\\n\", selected_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e85a39a-2653-4874-943c-ad3458c2bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_directory(directory):\n",
    "    if directory.exists() and directory.is_dir():\n",
    "        for file in directory.iterdir():\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "            elif file.is_dir():\n",
    "                import shutil\n",
    "                shutil.rmtree(file)\n",
    "\n",
    "clear_directory(TARGET_IMG_DIR)\n",
    "clear_directory(TARGET_ANN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbdf8f22-211c-4f5d-9410-c22eeb732c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for breed in selected_breeds:\n",
    "    breed_img_dir = IMAGES_DIR / breed\n",
    "    breed_ann_dir = ANNOTATIONS_DIR / breed\n",
    "\n",
    "    imgs = list(breed_img_dir.glob(\"*.jpg\"))\n",
    "    # for trial 2, increased image sampling from 100 to 175 per breed\n",
    "    # for trial 3, removed image cap per breed\n",
    "    # selected_imgs = random.sample(imgs, min(175, len(imgs)))\n",
    "    selected_imgs = imgs\n",
    "\n",
    "    for img_path in selected_imgs:\n",
    "        # Copy image\n",
    "        dest_img_dir = TARGET_IMG_DIR / breed\n",
    "        dest_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(img_path, dest_img_dir / img_path.name)\n",
    "\n",
    "        # Find corresponding annotation \n",
    "        base_name = img_path.stem  # n02085782_2\n",
    "        annot_path = breed_ann_dir / base_name \n",
    "\n",
    "        if annot_path.exists():\n",
    "            dest_ann_dir = TARGET_ANN_DIR / breed\n",
    "            dest_ann_dir.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy(annot_path, dest_ann_dir / annot_path.name)\n",
    "        else:\n",
    "            print(f\"Annotation missing for image {img_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa3f58-1db1-426b-b974-bb2e870cf90d",
   "metadata": {},
   "source": [
    "## TRIAL 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29655f81-fee8-403c-a0b1-ae24f9062f39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140fa2d7-54db-415a-9c79-48d3b166b588",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in subset: 4699\n",
      "Number of classes: 30\n",
      "Class names: ['n02086240-Shih-Tzu', 'n02087394-Rhodesian_ridgeback', 'n02088466-bloodhound', 'n02090379-redbone', 'n02091467-Norwegian_elkhound', 'n02091635-otterhound', 'n02092339-Weimaraner', 'n02094433-Yorkshire_terrier', 'n02096177-cairn', 'n02096437-Dandie_Dinmont', 'n02097047-miniature_schnauzer', 'n02098105-soft-coated_wheaten_terrier', 'n02098413-Lhasa', 'n02100735-English_setter', 'n02102480-Sussex_spaniel', 'n02104365-schipperke', 'n02105251-briard', 'n02105641-Old_English_sheepdog', 'n02106166-Border_collie', 'n02106662-German_shepherd', 'n02107142-Doberman', 'n02107574-Greater_Swiss_Mountain_dog', 'n02107683-Bernese_mountain_dog', 'n02108000-EntleBucher', 'n02108089-boxer', 'n02108422-bull_mastiff', 'n02109047-Great_Dane', 'n02109525-Saint_Bernard', 'n02112137-chow', 'n02113624-toy_poodle']\n",
      "Train size: 3759, Test size: 940\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize images to 224x224\\\n",
    "    transforms.ToTensor(),            # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset from subset folder\n",
    "dataset = ImageFolder(root=str(TARGET_IMG_DIR), transform=transform)\n",
    "\n",
    "print(f\"Total images in subset: {len(dataset)}\")\n",
    "print(f\"Number of classes: {len(dataset.classes)}\")\n",
    "print(f\"Class names: {dataset.classes}\")\n",
    "\n",
    "# Split dataset into train and test sets 80% train, 20% test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dfc3f-dc53-48c2-b523-dd7443662c30",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984a2fad-fa90-478a-8a0a-6b0a643e81d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleDogCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleDogCNN(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(SimpleDogCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # input RGB channels=3\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # downsample by factor of 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # assuming input size 224x224\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (224 -> 112)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (112 -> 56)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # (56 -> 28)\n",
    "        x = x.view(-1, 128 * 28 * 28)         # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model and send to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleDogCNN(num_classes=30).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73bebe2b-a86c-4831-b294-0aab00898374",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7284adf1-5ab7-4dd9-a3df-6d6b17edb296",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train loss: 3.4654, Train acc: 0.0327 | Val loss: 3.3995, Val acc: 0.0457 | Time: 315.4s\n",
      "Epoch 2/10 - Train loss: 3.3946, Train acc: 0.0463 | Val loss: 3.5048, Val acc: 0.0330 | Time: 289.1s\n",
      "Epoch 3/10 - Train loss: 3.3038, Train acc: 0.0785 | Val loss: 3.2556, Val acc: 0.0819 | Time: 360.7s\n",
      "Epoch 4/10 - Train loss: 2.9217, Train acc: 0.1785 | Val loss: 3.4022, Val acc: 0.0798 | Time: 310.7s\n",
      "Epoch 5/10 - Train loss: 1.9219, Train acc: 0.4605 | Val loss: 4.0709, Val acc: 0.0798 | Time: 314.5s\n",
      "Epoch 6/10 - Train loss: 0.5619, Train acc: 0.8476 | Val loss: 6.9316, Val acc: 0.0872 | Time: 356.4s\n",
      "Epoch 7/10 - Train loss: 0.1074, Train acc: 0.9726 | Val loss: 8.6180, Val acc: 0.0979 | Time: 333.1s\n",
      "Epoch 8/10 - Train loss: 0.0382, Train acc: 0.9910 | Val loss: 9.7904, Val acc: 0.0979 | Time: 342.5s\n",
      "Epoch 9/10 - Train loss: 0.0228, Train acc: 0.9957 | Val loss: 9.7120, Val acc: 0.0894 | Time: 294.0s\n",
      "Epoch 10/10 - Train loss: 0.0064, Train acc: 0.9989 | Val loss: 10.6442, Val acc: 0.0872 | Time: 310.9s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 10\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = eval_model(model, test_loader, criterion, device)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} | \"\n",
    "          f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda278d-d27c-437a-940b-adf2e496ee47",
   "metadata": {},
   "source": [
    "Clear overfitting - high training acc but low val acc and high val loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f79aa-5a2d-4847-9c9e-82dfe5f52051",
   "metadata": {},
   "source": [
    "## TRIAL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67335bc-45a2-4740-bf32-773423f6b650",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Only normalization for validation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "TARGET_IMG_DIR = Path(\"working_data/images\")\n",
    "\n",
    "# Load datasets\n",
    "full_dataset = ImageFolder(root=str(TARGET_IMG_DIR), transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Apply test transforms to test dataset manually\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91470ba6-9e17-40f7-9b63-84afa833f684",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDogCNN(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(SimpleDogCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleDogCNN(num_classes=30).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a89454-1388-4082-9f3f-4d8d9c3a856e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d180e66d-c8fd-4629-89ca-15b4e94ecbac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=40):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "              f\"Time: {elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657790f-131b-49ef-bdc3-a8888aa36f42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Trial 2 before I added in time stats and increased image sampling to 175 from 100\n",
    "\n",
    "# Epoch 1/10 - Train Loss: 3.3983, Train Acc: 0.0423 - Val Loss: 3.3788, Val Acc: 0.0553\n",
    "# Epoch 2/10 - Train Loss: 3.3586, Train Acc: 0.0521 - Val Loss: 3.3306, Val Acc: 0.0532\n",
    "# Epoch 3/10 - Train Loss: 3.3272, Train Acc: 0.0660 - Val Loss: 3.3127, Val Acc: 0.0628\n",
    "# Epoch 4/10 - Train Loss: 3.2889, Train Acc: 0.0766 - Val Loss: 3.2833, Val Acc: 0.0617\n",
    "# Epoch 5/10 - Train Loss: 3.2566, Train Acc: 0.0793 - Val Loss: 3.2485, Val Acc: 0.0691\n",
    "# Epoch 6/10 - Train Loss: 3.2060, Train Acc: 0.0950 - Val Loss: 3.2163, Val Acc: 0.0830\n",
    "# Epoch 7/10 - Train Loss: 3.1721, Train Acc: 0.1019 - Val Loss: 3.1841, Val Acc: 0.0755\n",
    "# Epoch 8/10 - Train Loss: 3.1335, Train Acc: 0.1016 - Val Loss: 3.1591, Val Acc: 0.0926\n",
    "\n",
    "# Restarted kernel and re ran after change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ffc463-5be0-4af9-acd6-eb353b9d826a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Train Loss: 3.3915, Train Acc: 0.0446 - Val Loss: 3.3443, Val Acc: 0.0614 - Time: 226.99s\n",
      "Epoch 2/40 - Train Loss: 3.3368, Train Acc: 0.0575 - Val Loss: 3.2970, Val Acc: 0.0833 - Time: 222.52s\n",
      "Epoch 3/40 - Train Loss: 3.3114, Train Acc: 0.0662 - Val Loss: 3.2711, Val Acc: 0.0793 - Time: 221.47s\n",
      "Epoch 4/40 - Train Loss: 3.2844, Train Acc: 0.0692 - Val Loss: 3.2384, Val Acc: 0.0793 - Time: 220.76s\n",
      "Epoch 5/40 - Train Loss: 3.2537, Train Acc: 0.0811 - Val Loss: 3.2062, Val Acc: 0.0842 - Time: 220.60s\n",
      "Epoch 6/40 - Train Loss: 3.2075, Train Acc: 0.0883 - Val Loss: 3.1796, Val Acc: 0.1011 - Time: 226.24s\n",
      "Epoch 7/40 - Train Loss: 3.1684, Train Acc: 0.1069 - Val Loss: 3.1580, Val Acc: 0.1110 - Time: 225.21s\n",
      "Epoch 8/40 - Train Loss: 3.1257, Train Acc: 0.1108 - Val Loss: 3.1218, Val Acc: 0.1100 - Time: 255.59s\n",
      "Epoch 9/40 - Train Loss: 3.0865, Train Acc: 0.1128 - Val Loss: 3.0657, Val Acc: 0.1229 - Time: 313.17s\n",
      "Epoch 10/40 - Train Loss: 3.0361, Train Acc: 0.1255 - Val Loss: 3.1000, Val Acc: 0.1269 - Time: 283.30s\n",
      "Epoch 11/40 - Train Loss: 2.9910, Train Acc: 0.1274 - Val Loss: 3.0007, Val Acc: 0.1437 - Time: 312.61s\n",
      "Epoch 12/40 - Train Loss: 2.9628, Train Acc: 0.1411 - Val Loss: 3.0371, Val Acc: 0.1308 - Time: 318.12s\n",
      "Epoch 13/40 - Train Loss: 2.9366, Train Acc: 0.1550 - Val Loss: 2.9778, Val Acc: 0.1447 - Time: 316.76s\n",
      "Epoch 14/40 - Train Loss: 2.9263, Train Acc: 0.1532 - Val Loss: 2.9662, Val Acc: 0.1615 - Time: 311.55s\n",
      "Epoch 15/40 - Train Loss: 2.8909, Train Acc: 0.1572 - Val Loss: 2.9833, Val Acc: 0.1556 - Time: 324.15s\n",
      "Epoch 16/40 - Train Loss: 2.8723, Train Acc: 0.1579 - Val Loss: 2.9402, Val Acc: 0.1675 - Time: 294.62s\n",
      "Epoch 17/40 - Train Loss: 2.8539, Train Acc: 0.1689 - Val Loss: 2.9503, Val Acc: 0.1645 - Time: 279.35s\n",
      "Epoch 18/40 - Train Loss: 2.8503, Train Acc: 0.1689 - Val Loss: 2.9188, Val Acc: 0.1675 - Time: 287.09s\n",
      "Epoch 19/40 - Train Loss: 2.8348, Train Acc: 0.1733 - Val Loss: 2.9035, Val Acc: 0.1655 - Time: 283.36s\n",
      "Epoch 20/40 - Train Loss: 2.8246, Train Acc: 0.1790 - Val Loss: 2.9328, Val Acc: 0.1675 - Time: 290.22s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1a1ba-9091-4fa5-bc5c-95defd065706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel interrupted, plateauing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ffa6c-4cc6-4765-b25d-2e2945843d67",
   "metadata": {},
   "source": [
    "## TRIAL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c2a6b3-8472-416a-8b09-a72f1c6ddfaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Only normalization for validation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "TARGET_IMG_DIR = Path(\"working_data/images\")\n",
    "\n",
    "# Load datasets\n",
    "full_dataset = ImageFolder(root=str(TARGET_IMG_DIR), transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Apply test transforms to test dataset manually\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "538c32d2-65c0-4cdf-8209-84e254ae7a8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes=30).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1ec630-34a7-44b1-aa49-0a700cc1dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c977b2-4ce8-4625-abbd-a77683750663",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "              f\"Time: {elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53f21bc-4930-484a-9562-f3e475a6d065",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.9435, Train Acc: 0.1083 - Val Loss: 2.9286, Val Acc: 0.1221 - Time: 367.24s\n",
      "Epoch 2/20 - Train Loss: 2.7981, Train Acc: 0.1407 - Val Loss: 2.8358, Val Acc: 0.1366 - Time: 314.17s\n",
      "Epoch 3/20 - Train Loss: 2.6887, Train Acc: 0.1901 - Val Loss: 2.8754, Val Acc: 0.1323 - Time: 330.19s\n",
      "Epoch 4/20 - Train Loss: 2.5946, Train Acc: 0.2119 - Val Loss: 2.7968, Val Acc: 0.1599 - Time: 337.97s\n",
      "Epoch 5/20 - Train Loss: 2.5307, Train Acc: 0.2308 - Val Loss: 2.7138, Val Acc: 0.1831 - Time: 368.57s\n",
      "Epoch 6/20 - Train Loss: 2.3889, Train Acc: 0.2755 - Val Loss: 2.4585, Val Acc: 0.2369 - Time: 323.54s\n",
      "Epoch 7/20 - Train Loss: 2.3211, Train Acc: 0.2886 - Val Loss: 2.4259, Val Acc: 0.2355 - Time: 300.11s\n",
      "Epoch 8/20 - Train Loss: 2.2712, Train Acc: 0.3108 - Val Loss: 2.3658, Val Acc: 0.2747 - Time: 262.19s\n",
      "Epoch 9/20 - Train Loss: 2.2134, Train Acc: 0.3264 - Val Loss: 2.3587, Val Acc: 0.2544 - Time: 299.17s\n",
      "Epoch 10/20 - Train Loss: 2.1927, Train Acc: 0.3326 - Val Loss: 2.3051, Val Acc: 0.2965 - Time: 298.74s\n",
      "Epoch 11/20 - Train Loss: 2.0845, Train Acc: 0.3740 - Val Loss: 2.2291, Val Acc: 0.3023 - Time: 310.16s\n",
      "Epoch 12/20 - Train Loss: 2.0429, Train Acc: 0.3773 - Val Loss: 2.2058, Val Acc: 0.2965 - Time: 289.59s\n",
      "Epoch 13/20 - Train Loss: 1.9986, Train Acc: 0.3999 - Val Loss: 2.2400, Val Acc: 0.2922 - Time: 308.23s\n",
      "Epoch 14/20 - Train Loss: 1.9824, Train Acc: 0.3966 - Val Loss: 2.2226, Val Acc: 0.3067 - Time: 311.24s\n",
      "Epoch 15/20 - Train Loss: 1.9507, Train Acc: 0.4115 - Val Loss: 2.1943, Val Acc: 0.3387 - Time: 306.91s\n",
      "Epoch 16/20 - Train Loss: 1.8935, Train Acc: 0.4424 - Val Loss: 2.0977, Val Acc: 0.3387 - Time: 303.17s\n",
      "Epoch 17/20 - Train Loss: 1.8685, Train Acc: 0.4446 - Val Loss: 2.1526, Val Acc: 0.3343 - Time: 308.53s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n\u001b[1;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn4(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x))))\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_avg_pool(x)\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f7df2a1-2f24-40e4-a394-50d0823d8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel interrupted, plateauing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a323c5-76cd-45df-8f66-32f3ec3f7fff",
   "metadata": {},
   "source": [
    "## TRIAL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72b39dc5-1c28-4045-8e64-aa9ac294b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Only normalization for validation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "TARGET_IMG_DIR = Path(\"working_data/images\")\n",
    "\n",
    "full_dataset = ImageFolder(root=str(TARGET_IMG_DIR), transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn4   = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(num_classes=20).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=20, early_stop_patience=5):\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Scheduler step on validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "              f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}. Best Val Acc: {best_val_acc:.4f} at epoch {best_epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21860a48-a894-45c2-b15f-cb3bc303674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.9203, Train Acc: 0.1087 - Val Loss: 2.8305, Val Acc: 0.1279 - Time: 526.94s\n",
      "Epoch 2/20 - Train Loss: 2.8011, Train Acc: 0.1410 - Val Loss: 2.8425, Val Acc: 0.1497 - Time: 568.67s\n",
      "Epoch 3/20 - Train Loss: 2.7114, Train Acc: 0.1625 - Val Loss: 2.7429, Val Acc: 0.1483 - Time: 494.88s\n",
      "Epoch 4/20 - Train Loss: 2.6270, Train Acc: 0.1890 - Val Loss: 2.6551, Val Acc: 0.2035 - Time: 383.16s\n",
      "Epoch 5/20 - Train Loss: 2.5681, Train Acc: 0.2028 - Val Loss: 2.5625, Val Acc: 0.2166 - Time: 376.42s\n",
      "Epoch 6/20 - Train Loss: 2.4735, Train Acc: 0.2210 - Val Loss: 2.6325, Val Acc: 0.2078 - Time: 424.73s\n",
      "Epoch 7/20 - Train Loss: 2.4458, Train Acc: 0.2530 - Val Loss: 2.6531, Val Acc: 0.1933 - Time: 492.26s\n",
      "Epoch 8/20 - Train Loss: 2.3818, Train Acc: 0.2585 - Val Loss: 2.4878, Val Acc: 0.2267 - Time: 337.27s\n",
      "Epoch 9/20 - Train Loss: 2.3395, Train Acc: 0.2646 - Val Loss: 2.3542, Val Acc: 0.2980 - Time: 347.50s\n",
      "Epoch 10/20 - Train Loss: 2.2636, Train Acc: 0.2973 - Val Loss: 2.4656, Val Acc: 0.2544 - Time: 363.44s\n",
      "Epoch 11/20 - Train Loss: 2.2335, Train Acc: 0.3028 - Val Loss: 2.5350, Val Acc: 0.2253 - Time: 367.28s\n",
      "Epoch 12/20 - Train Loss: 2.2167, Train Acc: 0.3021 - Val Loss: 2.2709, Val Acc: 0.3110 - Time: 384.08s\n",
      "Epoch 13/20 - Train Loss: 2.1203, Train Acc: 0.3424 - Val Loss: 2.2008, Val Acc: 0.3140 - Time: 437.84s\n",
      "Epoch 14/20 - Train Loss: 2.1132, Train Acc: 0.3388 - Val Loss: 2.3924, Val Acc: 0.2922 - Time: 414.53s\n",
      "Epoch 15/20 - Train Loss: 2.1079, Train Acc: 0.3344 - Val Loss: 2.5016, Val Acc: 0.2456 - Time: 285.22s\n",
      "Epoch 16/20 - Train Loss: 2.0462, Train Acc: 0.3573 - Val Loss: 2.3230, Val Acc: 0.2689 - Time: 334.33s\n",
      "Epoch 17/20 - Train Loss: 2.0147, Train Acc: 0.3595 - Val Loss: 2.2265, Val Acc: 0.3270 - Time: 365.79s\n",
      "Epoch 18/20 - Train Loss: 1.9908, Train Acc: 0.3711 - Val Loss: 2.1411, Val Acc: 0.3314 - Time: 342.46s\n",
      "Epoch 19/20 - Train Loss: 1.9477, Train Acc: 0.3908 - Val Loss: 2.2441, Val Acc: 0.3372 - Time: 274.27s\n",
      "Epoch 20/20 - Train Loss: 1.9103, Train Acc: 0.3948 - Val Loss: 2.2513, Val Acc: 0.2922 - Time: 234.25s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0520ea6f-2b40-485e-8fba-ff4aac792c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
